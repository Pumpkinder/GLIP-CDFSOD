# ü•á NTIRE 2025 CD-FSOD Challenge @ CVPR Workshop

We are the **award-winning team** of the **NTIRE 2025 Cross-Domain Few-Shot Object Detection (CD-FSOD) Challenge** at the **CVPR Workshop**.

- üèÜ **Track**: `open-source track`
- üéñÔ∏è **Award**: **3rd Place**

üîó [NTIRE 2025 Official Website](https://cvlai.net/ntire/2025/)  
üîó [NTIRE 2025 Challenge Website](https://codalab.lisn.upsaclay.fr/competitions/21851)  
üîó [CD-FSOD Challenge Repository](https://github.com/lovelyqian/NTIRE2025_CDFSOD)

![CD-FSOD Task](https://upload-images.jianshu.io/upload_images/9933353-3d7be0d924bd4270.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


---

## üß† Overview

This repository contains our solution for the `open-source track` of the NTIRE 2025 CD-FSOD Challenge.  
We propose a **Pseudo-Label Driven Vision-Language Grounding method for CD-FSOD, the proposed method mainly combines large-scale foundation models with an iterative pseudo-labeling strategy**, which achieves strong performance on the challenge. 

![Overview](demo.jpg)

---

## üõ†Ô∏è Environment Setup

***Environment*** This repo requires Pytorch>=1.9 and torchvision. We recommand using docker to setup the environment. You can use this pre-built docker image or this one depending on your GPU.`docker pull pengchuanzhang/maskrcnn:ubuntu18-py3.7-cuda10.2-pytorch1.9``docker pull pengchuanzhang/pytorch:ubuntu20.04_torch1.9-cuda11.3-nccl2.9.9`

Then install the following packages:

```
pip install einops shapely timm yacs tensorboardX ftfy prettytable pymongo
pip install transformers 
python setup.py build develop --user
```


## üìÇ Dataset Preparation
Please follow the instructions in the [official CD-FSOD repo](https://github.com/lovelyqian/NTIRE2025_CDFSOD) to download and prepare the dataset.

## Model Zoo

Download the [weight](https://huggingface.co/GLIPModel/GLIP/blob/main/glip_large_model.pth) to the /GLIP/weights folder. The corresponding config file is [config](configs/pretrain/glip_Swin_L.yaml).

Reffering to [microsoft/GLIP: Grounded Language-Image Pre-training](https://github.com/microsoft/GLIP) for more information.

pretrained model: 

The **checkpoint** of the best fine-tuned model can be found here [Fine-tune model weights](https://huggingface.co/ZHENYU99/CDFSOD/tree/main).


## üîç Inference & Evaluation

Run the following command to evaluate on the dataset. Set `{config_file}`, `{model_checkpoint}` according to the `Model Zoo`. Set {odinw_configs} to the path of the task yaml.

```
python tools/test_grounding_net.py --config-file {config_file} --weight {model_checkpoint} \
      --task_config {odinw_configs} \
      TEST.IMS_PER_BATCH 1 SOLVER.IMS_PER_BATCH 1 \
      TEST.EVAL_TASK detection \
      DATASETS.TRAIN_DATASETNAME_SUFFIX _grounding \
      DATALOADER.DISTRIBUTE_CHUNK_AMONG_NODE False \
      DATASETS.USE_OVERRIDE_CATEGORY True \
      DATASETS.USE_CAPTION_PROMPT True
      
######### for example
python tools/test_grounding_net.py --config-file /root/autodl-tmp/GLIP/configs/pretrain/glip_Swin_L.yaml --weight /root/autodl-tmp/GLIP/Best_model/data1_1s_full_pse0.4-0.5_1333.pth \
    --task_config /root/autodl-tmp/GLIP/configs/cdfsod/dataset1.yaml \
    TEST.IMS_PER_BATCH 1 SOLVER.Best_modelIMS_PER_BATCH 1 \
    TEST.EVAL_TASK detection \
    DATASETS.TRAIN_DATASETNAME_SUFFIX _grounding \
    DATALOADER.DISTRIBUTE_CHUNK_AMONG_NODE False \
    DATASETS.USE_OVERRIDE_CATEGORY True \
    DATASETS.USE_CAPTION_PROMPT True
```

**When testing on the 1-shot and 5-shot checkpoints of dataset3**, the additional parameter `MODEL.DYHEAD.FUSE_CONFIG.ADD_LINEAR_LAYER` should be set to **True**, as:

```
python tools/test_grounding_net.py --config-file {config_file} --weight {model_checkpoint} \
      --task_config {odinw_configs} \
      TEST.IMS_PER_BATCH 1 SOLVER.IMS_PER_BATCH 1 \
      TEST.EVAL_TASK detection \
      DATASETS.TRAIN_DATASETNAME_SUFFIX _grounding \
      DATALOADER.DISTRIBUTE_CHUNK_AMONG_NODE False \
      DATASETS.USE_OVERRIDE_CATEGORY True \
      DATASETS.USE_CAPTION_PROMPT True\
      MODEL.DYHEAD.FUSE_CONFIG.ADD_LINEAR_LAYER True
```

The generated prediction JSON file is located in the /OUTPUT/eval folder. **For dataset1, the category labels in the bbox.json prediction file generated by GLIP are 1-7. Please replace all "category_id": 7 with "category_id": 0 in the file.**

## Fine-Tuning

Due to randomness in GLIP, the results obtained from fine-tuning may vary each time. Therefore, please run the process multiple times under different seed to achieve optimal performance. Also, **please remember to clean up the files in the OUTPUT directory**, as multiple checkpoints will be saved during the fine-tuning process. We usually use the last checkpoint for testing. We use 4 vGPU-32GB for fine-tuning.


The generated prediction JSON file is located in the /OUTPUT/eval folder. The corresponding weight is in the OUTPUT/ft_task_{N} folder. **Before each fine-tuning, please save and delete the existing contents in these directories**.



#### dataset1

Firstly, change the path in every data1_{N}shot_process.sh: ` DIR=/root/autodl-tmp/GLIP`  to your path.

```
bash data1_1shot_process.sh
bash data1_5shot_process.sh
bash data1_10shot_process.sh
```

**For dataset1, the category labels in the bbox.json prediction file generated by GLIP are 1-7. Please replace all "category_id": 7 with "category_id": 0 in the file.**

#### dataset2

Firstly, change the path in every data2_{N}shot_process.sh: ` DIR=/root/autodl-tmp/GLIP`  to your path.

```
bash data2_1shot_process.sh
bash data2_5shot_process.sh
bash data2_10shot_process.sh
```

#### dataset3

Firstly, change the path in every data3_{N}shot_process.sh: ` DIR=/root/autodl-tmp/GLIP`  to your path.

```
bash data3_1shot_process.sh
bash data3_5shot_process.sh
bash data3_10shot_process.sh
```

## Add GroundingDino

#### Get GroundingDino Result

Refer to [DINO_CDFSOD](https://github.com/Pumpkinder/CDFSOD.git)  .

#### Ensemble

Place the JSON files of the GLIP model and the DINO model into the `/GLIP_result`and `/DINO_result` folders, respectively. Run:

```
python ensemble.py
```

The final results will be saved in the `/ensemble_results` folder.

## Acknowledgement

We express our gratitude to the [GLIP](https://github.com/microsoft/GLIP) and [Open-GroundingDino](https://github.com/longzw1997/Open-GroundingDino) authors for their open-source contribution.

## üìÑ Citation
If you use our method or codes in your research, please cite:
```
@inproceedings{fu2025ntire, 
  title={NTIRE 2025 challenge on cross-domain few-shot object detection: methods and results,
  author={Fu, Yuqian and Qiu, Xingyu and Ren, Bin and Fu, Yanwei and Timofte, Radu and Sebe, Nicu and Yang, Ming-Hsuan and Van Gool, Luc and others},
  booktitle={CVPRW},
  year={2025}
}
```





