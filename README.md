



# Pseudo-label GLIP+



Our code is based on [microsoft/GLIP: Grounded Language-Image Pre-training](https://github.com/microsoft/GLIP). And [ECCV 2024\] Official implementation of the paper "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"](https://github.com/IDEA-Research/GroundingDINO)



## Installation and Setup

***Environment*** This repo requires Pytorch>=1.9 and torchvision. We recommand using docker to setup the environment. You can use this pre-built docker image or this one depending on your GPU.`docker pull pengchuanzhang/maskrcnn:ubuntu18-py3.7-cuda10.2-pytorch1.9``docker pull pengchuanzhang/pytorch:ubuntu20.04_torch1.9-cuda11.3-nccl2.9.9`

Then install the following packages:

```
pip install einops shapely timm yacs tensorboardX ftfy prettytable pymongo
pip install transformers 
python setup.py build develop --user
```

Reffering to [microsoft/GLIP: Grounded Language-Image Pre-training](https://github.com/microsoft/GLIP) for more information.



## DATSET

Place the entire folders of dataset1, dataset2, and dataset3 into the DATASET directory.





## Model Zoo

Download the [weight](https://huggingface.co/GLIPModel/GLIP/blob/main/glip_large_model.pth) to the /GLIP/weights folder. The corresponding config file is [config](configs/pretrain/glip_Swin_L.yaml).

Reffering to [microsoft/GLIP: Grounded Language-Image Pre-training](https://github.com/microsoft/GLIP) for more information.



## Checkpoint

The checkpoint of the best fine-tuned model can be found here [Fine-tune model weights](https://huggingface.co/ZHENYU99/CDFSOD/tree/main).



## Evaluation

Run the following command to evaluate on the dataset. Set `{config_file}`, `{model_checkpoint}` according to the `Model Zoo`. Set {odinw_configs} to the path of the task yaml.



```
python tools/test_grounding_net.py --config-file {config_file} --weight {model_checkpoint} \
      --task_config {odinw_configs} \
      TEST.IMS_PER_BATCH 1 SOLVER.IMS_PER_BATCH 1 \
      TEST.EVAL_TASK detection \
      DATASETS.TRAIN_DATASETNAME_SUFFIX _grounding \
      DATALOADER.DISTRIBUTE_CHUNK_AMONG_NODE False \
      DATASETS.USE_OVERRIDE_CATEGORY True \
      DATASETS.USE_CAPTION_PROMPT True
      
######### for example
python tools/test_grounding_net.py --config-file /root/autodl-tmp/GLIP/configs/pretrain/glip_Swin_L.yaml --weight /root/autodl-tmp/GLIP/Best_model/data1_1s_full_pse0.4-0.5_1333.pth \
    --task_config /root/autodl-tmp/GLIP/configs/cdfsod/dataset1.yaml \
    TEST.IMS_PER_BATCH 1 SOLVER.Best_modelIMS_PER_BATCH 1 \
    TEST.EVAL_TASK detection \
    DATASETS.TRAIN_DATASETNAME_SUFFIX _grounding \
    DATALOADER.DISTRIBUTE_CHUNK_AMONG_NODE False \
    DATASETS.USE_OVERRIDE_CATEGORY True \
    DATASETS.USE_CAPTION_PROMPT True
```



**When testing on the 1-shot and 5-shot checkpoints of dataset3**, the additional parameter `MODEL.DYHEAD.FUSE_CONFIG.ADD_LINEAR_LAYER` should be set to **True**, as:

```
python tools/test_grounding_net.py --config-file {config_file} --weight {model_checkpoint} \
      --task_config {odinw_configs} \
      TEST.IMS_PER_BATCH 1 SOLVER.IMS_PER_BATCH 1 \
      TEST.EVAL_TASK detection \
      DATASETS.TRAIN_DATASETNAME_SUFFIX _grounding \
      DATALOADER.DISTRIBUTE_CHUNK_AMONG_NODE False \
      DATASETS.USE_OVERRIDE_CATEGORY True \
      DATASETS.USE_CAPTION_PROMPT True\
      MODEL.DYHEAD.FUSE_CONFIG.ADD_LINEAR_LAYER True
```

The generated prediction JSON file is located in the /OUTPUT/eval folder. **For dataset1, the category labels in the bbox.json prediction file generated by GLIP are 1-7. Please replace all "category_id": 7 with "category_id": 0 in the file.**



## Fine-Tuning

Due to randomness in GLIP, the results obtained from fine-tuning may vary each time. Therefore, please run the process multiple times under different seed to achieve optimal performance. Also, **please remember to clean up the files in the OUTPUT directory**, as multiple checkpoints will be saved during the fine-tuning process. We usually use the last checkpoint for testing. We use 4 vGPU-32GB for fine-tuning.



The generated prediction JSON file is located in the /OUTPUT/eval folder. The corresponding weight is in the OUTPUT/ft_task_{N} folder. **Before each fine-tuning, please save and delete the existing contents in these directories**.



#### dataset1

Firstly, change the path in every data1_{N}shot_process.sh: ` DIR=/root/autodl-tmp/GLIP`  to your path.

```
bash data1_1shot_process.sh
bash data1_5shot_process.sh
bash data1_10shot_process.sh
```

**For dataset1, the category labels in the bbox.json prediction file generated by GLIP are 1-7. Please replace all "category_id": 7 with "category_id": 0 in the file.**

#### dataset2

Firstly, change the path in every data2_{N}shot_process.sh: ` DIR=/root/autodl-tmp/GLIP`  to your path.

```
bash data2_1shot_process.sh
bash data2_5shot_process.sh
bash data2_10shot_process.sh
```

#### dataset3

Firstly, change the path in every data3_{N}shot_process.sh: ` DIR=/root/autodl-tmp/GLIP`  to your path.

```
bash data3_1shot_process.sh
bash data3_5shot_process.sh
bash data3_10shot_process.sh
```





## Add GroundingDino

#### Get GroundingDino Result

Refer to [DINO_CDFSOD](https://github.com/Pumpkinder/CDFSOD.git)  .

#### Ensemble

Place the JSON files of the GLIP model and the DINO model into the `/GLIP_result`and `/DINO_result` folders, respectively. Run:

```
python ensemble.py
```

The final results will be saved in the `/ensemble_results` folder.



## Acknowledgement

We express our gratitude to the [GLIP](https://github.com/microsoft/GLIP) and [Open-GroundingDino](https://github.com/longzw1997/Open-GroundingDino) authors for their open-source contribution.



